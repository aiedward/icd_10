# Auto-mapping ICD-10 using Radiological reports

## Problems
We know that radiological reports contain a lot of useful information. In the reports, radiologist describes findings and provides suggestions for a provisional diagnosis which could be related to the final diagnosis of patients. We see this potential pattern can help us to map ICD-10 (diagnosis) based on the knowledge derived from the radiological reports. However, the reports are in plain text which is pretty hard to extract information. We set up this sandbox git branch **"radio"** to play around how we could extract useful information from the reports and use the information to map a correct ICD-10

## Plan
1. Use NLP techniques to preprocess and extract relevant features from the reports.
2. Use the features to train machine learning models.
3. Evaluate and discuss the model performance.
4. Integrate the best (accuracy and clinical validity) to the main project (where multiple models use different dataset to predict ICD-10).

## Data type
1. Type of imaging: plain file, CT, MRI, etc (text).
2. Location of investigatin: head, chest, abdoment, etc (text).
3. Radiological findings: describe relevant (normal/abnormal) findings of the investigation results, suggest diagnosis or abnormalities related to the patient based on the evidences.

## Review matching patterns between the reports and ICD-10
### Diagnostic Patterns
Using AI to match diagnostics with ICD10 codes by analyzing different patterns and choosing the most probable result.
Patterns that can be found:

1. Exact Match
Words in Dx match exactly with the words in ICD10.

2. Word Stem
Words in Dx have the same stem as the words in ICD10 (eg. pleuritis and pleurisy OR neurocysticercosis and cysticercosis).

3. Short forms
Words in Dx are short forms of words in ICD10 (eg. TB and tuberculosis/tuberculous).

4. Similar Meanings
Words in Dx have similar meanings with another word in ICD10 (eg. seizure/spasm/epilepsy).

5. Location
Words that suggest location can be used to narrow the scope of possible Dx and the corresponding ICD10 (eg. cranial CT scan → cerebral).

6. Word Clusters
Words that are in the same cluster can be used to narrow the scope of possible Dx and the corresponding ICD10 (eg. pleural effusion → tuberculous pleurisy)

Optional Improvements
- auto-correct can be used to reduce spelling errors.
- medical dictionary can be used as database for medical terms and extracting relevant keywords.
- taking into account the probability of different ICD10 occurring in Thai population
- result generated by the AI comes with confidence score, so the results below certain threshold can be flagged down and reviewed by medical staffs.

### Types of algorithms
1. Edit distance based
algorithm computes the number of operations needed to transform one string (word) to another (like steps needed to go from point A to point B).

1.1) Hamming distance
algorithm overlays first string over second string and find variation. Ordering matters.
Eg. textdistance.hamming('arrow', 'arow') returns 3, while textdistance.hamming.normalized_similarity('arrow', 'arow') returns 0.4 (1-3/5).

1.2) Levenshtein distance
algorithm computes the number of edits (insertion/deletion/substitution) needed to transform first string to the second.
Eg. textdistance.levenshtein('arrow', 'arow') returns 1, while textdistance.levenshtein.normalized_similarity('arrow', 'arow') returns 0.8 (1-1/5).

1.3) Jaro-Winkler
algorithm assign scores to two strings, with higher score if, 1. same chraracters within half the longer string minus one (N/2 -1) distance away, and 2. same order of matching characters.
Eg. textdistance.jaro_winkler("mes", "messi") returns 0.86
textdistance.jaro_winkler("crate", "crat") returns 0.96
textdistance.jaro_winkler("crate", "atcr") returns 0.0

2. Token-based
2.1) Jaccard index
tokenizing the strings then comparing both strings using (S1∩S2)/(S1∪S2).
Eg. >> tokens_1 = "hello new world".split()
>> tokens_2 = "hello world".split()
>> textdistance.jaccard(tokens_1 , tokens_2)
0.666 (2/(3+2-2))

2.2) Sorensen-Dice
tokenizing the strings then comparing both strings by counting all the similar words (2x S1∩S2) and dividing it by all the words.
Eg. >> tokens_1 = "hello new world".split()
>> tokens_2 = "hello world".split()
>> textdistance.jaccard(tokens_1 , tokens_2)
0.8

3. Sequence-based
3.1) Ratcliff-Obershelp similarity
algorithm finds longest common substrings, removes them, separates into left and right, repeat until no common characters, then add the removed characters and divide by total characters.
Eg. >> string1, string2 = "i am going home", "gone home"
>> textdistance.ratcliff_obershelp(string1, string2)
0.66
